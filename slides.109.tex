\documentclass{beamer}
% \mode<presentation>
\setbeamertemplate{navigation symbols}{}
\let\tempone\itemize
\let\temptwo\enditemize
\newcommand{\din}{{d_{\mathrm{in}}}}
\newcommand{\dhid}{{d_{\mathrm{hid}}}}
\newcommand{\dwin}{{d_{\mathrm{win}}}}
\newcommand{\dout}{{d_{\mathrm{out}}}}
\newcommand{\demb}{{d_{\mathrm{emb}}}}

\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{0.5\baselineskip}}{\temptwo}
% \usepackage{beamerthemeshadow}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pgffor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{tikz,etoolbox}
\usepackage{tikz,amsmath,siunitx}
\usetikzlibrary{arrows,snakes,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\newcommand{\softmax}{\mathrm{softmax}}
\usepackage[normalem]{ulem}
\newcommand\tst{% thick strike through  %% from http://tex.stackexchange.com/questions/134088/mis-alignment-of-columns-in-tabular-environment-when-using-ulem-and-beamer
  \bgroup%
  \markoverwith{\textcolor{red}{\rule[1.1ex]{1pt}{0.8pt}}}%
  \ULon%
}

\usepackage{subcaption}
\usepackage[absolute,overlay]{textpos}
\usepackage{pgf}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{shapes,arrows,positioning,automata,positioning,spy,matrix,scopes,chains}
\newcommand{\digs}[2]{\hphantom{999}\llap{#1}\,+\,\hphantom{999}\llap{#2}}
\setbeamersize{text margin left=6mm}
\setbeamersize{text margin right=6mm}
\renewcommand{\insertnavigation}[1]{}
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\usefonttheme{professionalfonts}
\setbeamercovered{transparent}
\mode<presentation>
\linespread{1.25}
\DeclareMathOperator{\Tr}{Tr} 

\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[all,dvips]{xy}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{natbib}
\usepackage[labelformat=empty]{caption}
\newcommand{\air}{\vspace{0.25cm}}
\newcommand{\mair}{\vspace{-0.25cm}}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols
\renewcommand{\rmdefault}{crm}
\newcommand{\lnbrack}{{\normalfont [}}
\newcommand{\rnbrack}{{\normalfont ]}\thinspace}
\newcommand{\lbbrack}{\textcolor{red}{\textbf{[}}}
\newcommand{\rbbrack}{\textcolor{red}{\textbf{]}}\thinspace}
\definecolor{vermillion}{RGB}{213,94,0}
\newcommand{\given}{\,|\,}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{skyblue}{RGB}{86,180,233}
\definecolor{bluegreen}{RGB}{90,143,41}
% \definecolor{bluegreen}{RGB}{0,158,115}
\definecolor{myyellow}{RGB}{240,228,66} % i dunno if this is the same as standard yellow
\definecolor{myblue}{RGB}{0,114,178}
\definecolor{vermillion}{RGB}{213,94,0}
\definecolor{redpurple}{RGB}{204,121,167}
\definecolor{lightgrey}{RGB}{234,234,234}

\newcommand{\clust}{\ensuremath{\mathrm{clust}}}
\newcommand{\loc}{\ensuremath{\mathrm{loc}}}
\newcommand{\nicein}{\ensuremath{\,{\in}\,}}
\newcommand{\niceq}{\ensuremath{\,{=}\,}}
\newcommand{\uc}{\ensuremath{\mathrm{c}}}
\newcommand{\hc}{\boldh_{\uc}}
\newcommand{\cb}{\boldb_{\mathrm{\uc}}}
\newcommand{\cW}{\boldW_{\mathrm{\uc}}}
% \newcommand{\tanh}{\mathrm{tanh}}

\newcommand{\ha}{\boldh_{\ua}}
\newcommand{\hp}{\boldh_{\up}}

% \newcommand{\boldx}{\mathbf{x}}
\newcommand{\boldy}{\mathbf{y}}

% \newcommand{\hc}{\boldh_{\mathrm{c}}}


\def\kargmax{\operatornamewithlimits{K-arg\,max}}
%\DeclareMathOperator{\topK}{topK}
\def\topK{\operatornamewithlimits{topK}}
\DeclareMathOperator{\suk}{succ}
\newcommand{\longpfx}[1]{\ensuremath{w_1 \cdots w_{#1}}}
\newcommand{\longgoldpfx}[1]{\ensuremath{y_1 \cdots y_{#1}}}
\newcommand{\pfx}[1]{\ensuremath{w_{1:{#1}}}}
\newcommand{\goldpfx}[1]{\ensuremath{y_{1:{#1}}}}
\newcommand{\beampred}[2]{\ensuremath{\hat{y}_{1:{#1}}^{({#2})}}}
\newcommand{\boldx}{\mathbf{x}}

\usetikzlibrary{positioning}
% \setbeamerfont{alerted text}{series=\bfseries}
% \setbeamerfont{structure}{series=\bfseries}
% Needed for diakgrams.
\def\im#1#2{
  \node(#1) [scale=#2]{\pgfbox[center,top]{\pgfuseimage{#1}}
};}
% \input{pictures_header}


\title[Seq2seq]{CS 109b \\   RNNs and Language }


\author[Alexander Rush]{Alexander Rush  (@harvardnlp) \\  
% {\scriptsize  (with  Yoon Kim, Sam Wiseman, Hendrik Strobelt, Yuntian Deng, Allen Schmaltz) } \\
% \url{http://www.github.com/harvardnlp/seq2seq-talk/}
} 

% \institute[Harvard SEAS]{ \\
%   \begin{center}
%     \includegraphics[width=1.3cm]{seas}
%   \end{center}

% }
\date{}
% \usetheme{Madrid}

\newcommand{\enc}{\mathrm{src}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\yvec}{\mathbf{y}}
\newcommand{\wvec}{\mathbf{w}}
\newcommand{\cvec}{\mathbf{c}}
\newcommand{\zvec}{\mathbf{z}}
% \newcommand{\mcY}{\mathcal{Y}}
% \newcommand{\mcV}{\mathcal{V}}
\newcommand{\context}{\mathbf{w}_{\mathrm{c}}}
\newcommand{\embcontext}{\mathbf{\tilde{w}}_{\mathrm{c}}}
\newcommand{\inpcontext}{\mathbf{\tilde{x}}}
\newcommand{\start}{\mathbf{\tilde{y}}_{\mathrm{c0}}}
\newcommand{\End}{\mathrm{\texttt{</s>}}}

\newcommand{\Uvec}{\mathbf{U}}
\newcommand{\Evec}{\mathbf{E}}
\newcommand{\Gvec}{\mathbf{G}}
\newcommand{\Fvec}{\mathbf{F}}
\newcommand{\Pvec}{\mathbf{P}}
\newcommand{\pvec}{\mathbf{p}}
\newcommand{\Qvec}{\mathbf{Q}}
\newcommand{\Vvec}{\mathbf{V}}
\newcommand{\Wvec}{\mathbf{W}}
\newcommand{\hvec}{\mathbf{h}}
% \newcommand{\reals}{\mathbb{R}}

\newcommand{\Cite}[1]{{\footnotesize \citep{#1}}}
\newcommand{\TT}[1]{{\footnotesize\tt{#1}}}
\newcommand{\boldw}{\boldsymbol{w}}
\newcommand{\boldu}{\boldsymbol{u}}
\newcommand{\boldv}{\boldsymbol{v}}
\newcommand{\boldb}{\mathbf{b}}
\newcommand{\boldW}{\mathbf{W}}
\newcommand{\boldh}{\boldsymbol{h}}
\newcommand{\boldg}{\boldsymbol{g}}
\newcommand{\ua}{\ensuremath{\mathrm{a}}}
\newcommand{\up}{\ensuremath{\mathrm{p}}}
%\newcommand{\bphi}{\ensuremath{\mathbf{\phi}}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcA}{\mathcal{A}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\trans}{\ensuremath{\mathsf{T}}}
\def\argmin{\operatornamewithlimits{arg\,min}}
\def\argmax{\operatornamewithlimits{arg\,max}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}

\newcommand{\aphi}{\boldsymbol{\phi}_{\mathrm{a}}}
\newcommand{\pwphi}{\boldsymbol{\phi}_{\mathrm{p}}}
\newcommand{\squigaphi}{\widetilde{\boldsymbol{\phi}}_{\mathrm{a}}}
\newcommand{\squigpwphi}{\widetilde{\boldsymbol{\phi}}_{\mathrm{p}}}

\newcommand{\aW}{\boldW_{\mathrm{\ua}}}
\newcommand{\pW}{\boldW_{\mathrm{\up}}}

\newcommand{\ab}{\boldb_{\mathrm{\ua}}}
\newcommand{\pb}{\boldb_{\mathrm{\up}}}

\newcommand{\Da}{d_{\mathrm{a}}}
\newcommand{\Dp}{d_{\mathrm{p}}}

% \newcommand{\ha}{\boldh_{\ua}}
% \newcommand{\hp}{\boldh_{\up}}

\newcommand{\ourmodel}{This work}
\newcommand{\zro}{{\color{white}0}}


\def\argmax{\operatornamewithlimits{arg\,max}}
\def\kargmax{\operatornamewithlimits{K-arg\,max}}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Language Models}
  \end{center}
\end{frame}

\begin{frame}
  \only<1>{
  \begin{center}
    \includegraphics[width=3cm]{holmes}
  \end{center}
}
\only<2>{
  \begin{center}
    \begin{tikzpicture}[spy using outlines={circle, magnification=20,
        size=3cm, connect spies}]
      \node {\includegraphics[width=3cm]{holmes}}; 
      \spy [cyan] on
      (0.0,0.3) in node [left] at (5.4,-1.15);
    \end{tikzpicture}
  \end{center}
}
  \begin{quote}
    It is a capital mistake to theorize before one has
    data. Insensibly one begins to twist facts to suit theories,
    instead of theories to suit facts. -Sherlock Holmes, A Scandal in Bohemia
  \end{quote}


\end{frame}

\begin{frame}

  % \begin{center}
  %   \includegraphics[width=6cm]{lena}
  % \end{center}

  \air
  \only<1> {
    \begin{quote}
    It is a capital mistake to theorize before one has \_\_\_\_\_\_ $\ldots$ 
    \end{quote}
  }
  \only<2> {
    \begin{quote}
      108 938 285 28 184 29 593 219 58 772 \_\_\_\_\_\_ $\ldots$ 
    \end{quote}    
  }
\end{frame}



\begin{frame}
  \begin{center}
    \structure{Language Modeling Task}
  \end{center}
  Given a sequence of text give a probability distribution 
  over the next word. 

\air

  The Shannon game. Estimate the probability of the next letter/word
  given the previous.

  \begin{quote}
    THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG READING LAMP ON THE
    DESK SHED GLOW ON POLISHED \_\_\_\
  \end{quote}


\end{frame}


\begin{frame}
  \begin{center}
    \structure{Shannon (1948) \textit{Mathematical Model of Communication}}
  \end{center}
\air
  
\begin{quote}  
  We may consider a discrete source, therefore,
to be represented by a stochastic process. Conversely, any stochastic
process which produces a discrete sequence of symbols chosen from a finite
set may be considered a discrete source. This will include such cases as:

1. Natural written languages such as English, German, Chinese.
...
\end{quote}
\end{frame}

\begin{frame}[allowframebreaks]
  \begin{center}
    \structure{Shannon's Babblers}
  \end{center}
  \begin{quote}
   
  4. Third-order approximation (trigram structure as in English).

IN NO 1ST LAT WHEY CRATICT FROURE BIRS GROCID
PONDENOME OF DEMONSTURES OF THE REPTAGIN IS
REGOACTIONA OF CRE
  \end{quote}

  \begin{quote}
5. First-Order Word Approximation. Rather than continue with tetragram,
... , II-gram structure it is easier and better to jump at this
point to word units. Here words are chosen independently but with
their appropriate frequencies.

REPRESENTING AND SPEEDILY IS AN GOOD APT OR
COME CAN DIFFERENT NATURAL HERE HE THE A IN
CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES
THE LINE MESSAGE HAD BE THESE.

  \end{quote}

  \begin{quote}
    6. Second-Order Word Approximation. The word transition probabilities
are correct but no further structure is included.

THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH
'RITER THAT THE CHARACTER OF THIS POINT IS
THEREFORE ANOTHER METHOD FOR THE LETTERS
THAT THE TIME OF WHO EVER TOLD THE PROBLEM
FOR AN UNEXPECTED

The resemblance to ordinary English text increases quite noticeably at
each of the above steps.

  \end{quote}
\end{frame}

% \begin{frame}
%   \begin{center}
%     \textbf{Simple Language Models}
%   \end{center}
% \end{frame}


\begin{frame}

Goal: Estimate Markov model 
\[ p(w_{t} | w_{1}, \ldots, w_{t-1}) \approx p(w_{t} | w_{t-n}, \ldots w_{t-1})\] 

Ingredients: 

\begin{itemize}
\item 1 Corpus (e.g. the entire web)

\end{itemize}

Steps:

\begin{itemize}
\item (1) Collect words, (2) Count up n-grams, (3) Divide$^*$
  \begin{eqnarray*} 
    p(w_{t+1} | w_{t-n+1}, \ldots w_{t}) &=& \frac{\#( w_{t-n+1}, \ldots, w_{t+1}) }{\#( w_{t-n+1}, \ldots, w_{t})} \\
    &=&  \frac{\#(\mathrm{theorize\ before\ one\ has\ data})}{\#(\mathrm{theorize\ before\ one\ has})}
    \end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \includegraphics[width=\textwidth]{polished}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \alert{Google 1T}

  \end{center}
  \begin{table}
    \centering
  \begin{tabular}{ll}
    \toprule
    Number of token  &1,024,908,267,229 \\
    Number of sentences & 95,119,665,584 \\
    Size compressed (counts only) & 24 GB \\  
    \midrule
    Number of unigrams & 13,588,391 \\
    Number of bigrams & 314,843,401 \\ 
    Number of trigrams & 977,069,902 \\ 
    Number of fourgrams & 1,313,818,354 \\
    Number of fivegrams&  1,176,470,663 \\
    \bottomrule
  \end{tabular}
  \end{table}


\end{frame}

\begin{frame}
  \textbf{Zipf' Law (1935,1949):}
  \begin{quote}
    The frequency of any word is inversely proportional to its rank in the frequency table.
  \end{quote}


     \begin{center}
       \includegraphics[width=0.8\textwidth]{zipf}         
     \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{Neural Networks}
  \end{center}
\end{frame}

\begin{frame}{Intuition: N-Gram Issues}
  
  Training: 

  \begin{center}
    the arizona corporations commission \alert{authorized}
  \end{center}

  Test: 
  \begin{center}
    the colorado businesses organization \alert{\_\_\_}
  \end{center}
  \pause 
  
  \begin{itemize}
  \item Does this training example help here?
    \begin{itemize}
    \item Not really. No count overlap.
    \end{itemize}
    \air 
    \pause 
  \item Intuition: hope to learn that similar words act similarly. 
  \end{itemize}
\end{frame}


\begin{frame}{Alternative Approach}

  Treat as multi-class prediction!

  \air

  Let $\mcV$ be all possible words in the language (English 10,000 - 100,000). 
  Predict over words.

\air

 Important ideas that you have seen .

  \begin{enumerate}


  \item \textbf{Neural Network} to learn feature representation. 
  \item \textbf{Softmax} for multiclass prediction of next word (out of $\mcV$ )
  \item \textbf{Cross-entropy} loss as training objective.
  \end{enumerate}
\end{frame}


\begin{frame}{Language Modeling as Supervised Learning}
  \begin{quote}
    THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG READING LAMP ON THE
    DESK SHED GLOW ON POLISHED \_\_\_\
  \end{quote}


  \begin{itemize}
  \item Training data is pairs are $(w_t, \{w_{t-3}, w_{t-2}, w_{t-1}\})$.

  \item Input is sentence up until the blank, output is next word
    prediction.
  \item Challenging multi-class prediction problem, feature representation matters.

    \pause
    Neural Network 

    \[ p(w_t | \{w_{t-3}, w_{t-2}, w_{t-1}\} ) = \sigma(z)_{w_t} \]
    where 
    \[z = NN(\{w_{t-3}, w_{t-2}, w_{t-1}\}) \]
  \end{itemize}
\end{frame}

\begin{frame}
  A \structure{neural network} is a \alert{function approximator}

  
  \begin{itemize}
  \item  $NN(\boldx; \theta)$; a learned function from $\boldx$ with parameters $\theta$.
  \end{itemize}
% \end{frame}

  \pause
% \begin{frame}{Neural Network}

  \[NN_{MLP1}(\boldx) =  \boldW^2 \tanh(\boldW^1 \boldx + \boldb^1) + \boldb^2\]
  \begin{itemize}
  \item $\boldx$; input

  \item $\boldW, \boldb$; parameters
  % \item $\boldW^1 \in \reals^{\din \times \dhid}, \boldb^1 \in \reals^{1 \times \dhid}$; first affine transformation
  % \item $\boldW^2 \in \reals^{\dhid \times \dout}, \boldb^2 \in \reals^{1 \times \dout}$; second affine transformation
  \item $\tanh$ is \textit{non-linearity} 
  % \item $g(\boldx\boldW^1 + \boldb^1)$ is the \textit{hidden layer}
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=5cm]{tanh}     
  \end{figure}

\end{frame}



\begin{frame}{Word Representation}
  \begin{itemize}
  \item How can a word be fed into a neural network?

  \item Each word has an associated numerical index, not like images or 
    sounds.

  \item Need a way to know if two words are ``close'' to each other 
    for the network. 
  \end{itemize}

\end{frame}


\begin{frame}{Words Embeddings}
  \begin{center}
    \includegraphics[width=7cm]{emb}
  \end{center}
  \begin{itemize}
  \item Associate each words with an \textit{embedding} vector, e.g. in 50 dimensions. 
  \item Store this in a big table. 
  \item ``Move'' the vectors around based on backpropagation, i.e. if the model 
    makes the wrong prediction the vector associated with a word is updated.
  \end{itemize}
\end{frame}

% \begin{frame}{Word Embeddings}
  
% \end{frame}

% \begin{frame}
%   \begin{center}
%     \includegraphics[width=5cm]{brain}
%   \end{center}
% \end{frame}








% \begin{frame}
%   Logistic sigmoid function:
%   \[\sigma(t) = \frac{1}{1 + \exp(-t)} \]
%   \begin{figure}
%     \centering
%     \includegraphics[width=5cm]{sigmoid}     
%   \end{figure}

%   \begin{itemize}
%   \item $\sigma((\boldW^1 \boldx + \boldb^1)_i)$
%   \end{itemize}
% \end{frame}




% \begin{frame}{Softmax}
  
% \end{frame}



% \begin{frame}
%   \begin{center}


%   \begin{tikzpicture}
%     \node(aa)[draw, circle]{$x_1$};
%     \node(ab)[below of =  aa, draw, circle]{$x_2$};
%     \node(ac)[below of = ab, draw,circle]{$x_3$};
%     \node(ad)[below of = ac]{$\vdots$};
%     \node(ae)[below of =  ad, draw, circle]{$x_{\din}$};


%     \node(ba)[xshift= 2cm, yshift=-0.5cm, right of = aa, draw, circle]{$h_1$};
%     \node(bb)[below of =  ba, draw, circle]{$h_2$};
%     \node(bc)[below of = bb]{$\vdots$};

%     \node(bd)[below of =  bc, draw, circle]{$h_{\dhid}$};

%     \path[draw] (aa) -- (ba);
%     \path[draw] (aa) -- (bb);
%     \path[draw] (aa) -- (bd);

%     \path[draw] (ab) -- (ba);
%     \path[draw] (ab) -- (bb);
%     \path[draw] (ab) -- (bd);

%     \path[draw] (ac) -- (ba);
%     \path[draw] (ac) -- (bb);
%     \path[draw] (ac) -- (bd);

%     \path[draw] (ae) -- (ba);
%     \path[draw] (ae) -- (bb);
%     \path[draw] (ae) -- (bd);


%     \node(ca)[xshift= 2cm, yshift=0.5cm, right of = ba, draw, circle]{$z_1$};
%     \node(cb)[below of =  ca, draw, circle]{$z_2$};
%     \node(cc)[below of =  cb, draw, circle]{$z_3$};
%     \node(cd)[below of = cc]{$\vdots$};
%     \node(ce)[below of =  cd, draw, circle]{$z_{\dout}$};


%     \path[draw] (ba) -- (ca);
%     \path[draw] (ba) -- (cb);
%     \path[draw] (ba) -- (cc);
%     \path[draw] (ba) -- (ce);

%     \path[draw] (bb) -- (ca);
%     \path[draw] (bb) -- (cb);
%     \path[draw] (bb) -- (cc);
%     \path[draw] (bb) -- (ce);

%     \path[draw] (bd) -- (ca);
%     \path[draw] (bd) -- (cb);
%     \path[draw] (bd) -- (cc);
%     \path[draw] (bd) -- (ce);


%   \end{tikzpicture}
%   \end{center}j
% \end{frame}


% \begin{frame}{Interpretation of Network}

%   \begin{itemize}
%   \item Notation input $\reals^m$ and basis $\reals^d$.  
%   \item Unlike other problems here $m$ is large $|\mcV|$ (10,000-100,000)
%   \item But $d$ can be much smaller. Why is that?  
  
%   \air
%   \[\boldW^1 \begin{bmatrix}0 \\ 0\\ 1 \\ \vdots \\0 \\\end{bmatrix} = \boldW^1_{\star, j} \] 
      
%   \item The output of this layer $\boldW^1_{\star, j} \in \reals^d$ is called a \textit{word vector}. 
%   \end{itemize}
% \end{frame}

% \begin{frame}{Sparse versus Distributed Representation}
%   \begin{itemize}
%   \item Internally we have now seen two representations of words 

%   \item $\boldx$ Sparse:  Very high dimensional, easy to get back the original word, 
%     but feature weights are not shared between words. $\boldw_j$ is weight for 
%     one word. 
%     \[ [0, 0, 0, 1, \ldots 0] \] 
%     \pause 

%   \item $\bphi(\boldx)$ Distributed:  Low dimensional but dense, each dimension is a feature of the 
%     word, $\boldw_j$ is weight for basis function. 
%     \[ [0.23, 0.32, 0.109, -0.1231, \ldots, 0.402] \] 

%   \end{itemize}
% \end{frame}




% \begin{frame}
%   \begin{center}
%     \textbf{Neural Networks for Language}
%   \end{center}
  
%   \begin{itemize}
%   \item $\boldx$; previous words 
%   \item $NN(\boldx)$; score for next word  
%   \end{itemize}
%   \[NN_{MLP1}(\boldx) =  \boldW^2 \sigma(\boldW^1 \boldx + \boldb^1) + \boldb^2\]

%   \begin{center}
%     \begin{tikzpicture}
%     \node(aa)[draw, circle]{$x_1$};
%     \node(ab)[below of =  aa, draw, circle]{$x_2$};
%     \node(ac)[below of = ab, draw,circle]{$x_3$};
%     \node(ad)[below of = ac]{$\vdots$};
%     \node(ae)[below of =  ad, draw, circle]{$x_{n}$};


%     \node(ba)[xshift= 2cm, yshift=-0.5cm, right of = aa, draw, circle]{$h_1$};
%     \node(bb)[below of =  ba, draw, circle]{$h_2$};
%     \node(bc)[below of = bb]{$\vdots$};

%     \node(bd)[below of =  bc, draw, circle]{$h_{d}$};

%     \path[draw] (aa) -- (ba);
%     \path[draw] (aa) -- (bb);
%     \path[draw] (aa) -- (bd);

%     \path[draw] (ab) -- (ba);
%     \path[draw] (ab) -- (bb);
%     \path[draw] (ab) -- (bd);

%     \path[draw] (ac) -- (ba);
%     \path[draw] (ac) -- (bb);
%     \path[draw] (ac) -- (bd);

%     \path[draw] (ae) -- (ba);
%     \path[draw] (ae) -- (bb);
%     \path[draw] (ae) -- (bd);


%     \node(ca)[xshift= 2cm, yshift=0.5cm, right of = ba, draw]{word 1};
%     \node(cb)[below of =  ca, draw]{word 2};
%     \node(cc)[below of =  cb, draw]{word 3};
%     \node(cd)[below of = cc]{$\vdots$};
%     \node(ce)[below of =  cd, draw]{word n};


%     \path[draw] (ba) -- (ca);
%     \path[draw] (ba) -- (cb);
%     \path[draw] (ba) -- (cc);
%     \path[draw] (ba) -- (ce);

%     \path[draw] (bb) -- (ca);
%     \path[draw] (bb) -- (cb);
%     \path[draw] (bb) -- (cc);
%     \path[draw] (bb) -- (ce);

%     \path[draw] (bd) -- (ca);
%     \path[draw] (bd) -- (cb);
%     \path[draw] (bd) -- (cc);
%     \path[draw] (bd) -- (ce);
%   \end{tikzpicture}
% \end{center}
% \end{frame}



\begin{frame}
  \begin{center}
    \includegraphics[height=0.9\textheight]{graph}

    \href[pdfnewwindow=true]{http://harvardnlp.github.io/seq2seq-talk/web/wordvecs.html}{[Words Vectors]}
   \end{center}
\end{frame}


\begin{frame}{Notable Properties}
  After applying PCA to word embeddings
  \begin{center}
    \includegraphics[width=8cm]{athletes}
  \end{center}
\end{frame}




\begin{frame}{Modern Word Vectors}  
  [Demo: TensorBoard]
\end{frame}


\begin{frame}{Using For Language Modeling}
  Recall the definition of the softmax, with $z$ the representation 
  of the context.

  \[ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_{k}}}  \] 

  \air

  For language modeling $K$ is huge! Need to compute for the full
  vocabulary.
  \air

  Became possible to do with refinement of GPUs.
\end{frame}


\begin{frame}{Problems With MLP Language Models}  
  MLP language models provide a way to learn 
  representations of words.

  \begin{itemize}
  \item Can be trained as a standard neural network.
  \item Allows us to learn words are similar. 
  \end{itemize}

  \air

  Still have several \alert{issues}.

  \begin{itemize}
  \item Have to differentiate between locations (2 words back, 5 words back).
  \item Only have a fixed amount of history. 
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Recurrent Neural Networks Models}
  \end{center}
\end{frame}


\begin{frame}{RNN: Overview}
  
  \begin{itemize}
  \item Most successful deep learning model for language. 
    
  \item Deep over ``time''.  
  \end{itemize}
\end{frame}

% \begin{frame}
%     \begin{center}
%     \textbf{This Talk: Neural Networks For Language}
%   \end{center}
% \end{frame}

\begin{frame}{RNN}
  \begin{center}
    \includegraphics[width=11cm]{rnn}
  \end{center}  
\end{frame}

\begin{frame}
  RNN()
\end{frame}



\begin{frame}
  \begin{center}
    \alert{Seq2Seq Neural Network Toolbox}
    \air 
  \end{center}
  \begin{center}
    \begin{tabular}{cclll}
      \structure{Embeddings} & & sparse features &$\Rightarrow$& dense features \\\\
      \structure{RNNs} & & feature sequences & $\Rightarrow$ &dense features \\\\
      \structure{Softmax} & & dense features & $\Rightarrow$ & discrete predictions \\
    \end{tabular}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \begin{tabular}{cclll}
      \structure{RNNs/LSTMs} & & feature sequences & $\Rightarrow$ &dense features \\\\
    \end{tabular}
  \end{center}


  \begin{center}
    \includegraphics[width=11cm]{rnn}
  \end{center}  
\end{frame}



\begin{frame}
  \begin{center}
    \begin{tabular}{cclll}
      \structure{LM/Softmax} & & dense features & $\Rightarrow$ & discrete predictions \\
    \end{tabular}
    \air 

    \includegraphics[width=0.8\textwidth]{rnnlm5}
  \end{center}
  \[ p(w_t | w_1, \ldots, w_{t-1}; \theta) = \text{softmax}(\mathbf{W}_{out} \mathbf{h}_{t-1} + \mathbf{b}_{out}) \] 

  \[ p(w_{1:T} ) = \prod_{t} p(w_t | w_1, \ldots, w_{t-1}) \] 
    % \caption{Xu et al (2015)}  
\end{frame}

\begin{frame}
\begin{center}
Thanks!
\end{center}
\end{frame}


% \begin{frame}
%   \begin{center} 
%     \structure{Thank You}
%   \end{center}
%   \air 
%   \begin{center}
%     \includegraphics[width=3cm]{harvardnlp}
%   \end{center}
%   % \structure{Focus:} Deep learning of the representation of language structure

%   \begin{center}
%     \includegraphics[width=6cm]{harvardnlpgroup}
%   \end{center}
  
% \end{frame}



\begin{frame}[t,allowframebreaks]
  \frametitle{References}
  \begin{small}
    \bibliography{full,career2,seq2seqapps,ourwork,master,masterseqk,beamtrain}
  \end{small}
 \end{frame}

\bibliographystyle{apalike}

\end{document}
