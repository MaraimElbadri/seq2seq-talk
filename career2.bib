@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{Ammar2012,
abstract = {Although machine-learning applications are be-ing extensively used nowadays, there are too few students that are interested to pursue related stud-ies, such as computer science or artificial in-telligence. In this position paper we argue that an application-driven curriculum can serve to increase the interest of students to learn about machine learning. Such a curriculum uses well-established, profitable real-world applications as motivating examples during the lectures. We ex-amplify four real-world applications and poten-tial related class assignments.},
author = {Ammar, Haitham Bou and de Jong, S},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bou Ammar HAITHAMBOUAMMAR, Steven de Jong STEVENDEJONG - Unknown - Advocating an Application-driven Machine Learning Curriculum.pdf:pdf},
title = {{Advocating an Application-driven Machine Learning Curriculum}},
year = {2012}
}
@article{DBLP:journals/corr/AmodeiABCCCCCCD15,
author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan C and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni Y and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew Y and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
journal = {CoRR},
title = {{Deep Speech 2: End-to-End Speech Recognition in English and Mandarin}},
url = {http://arxiv.org/abs/1512.02595},
volume = {abs/1512.0},
year = {2015}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v1},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
eprint = {arXiv:1603.06042v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks.pdf:pdf},
journal = {arXiv},
month = {mar},
title = {{Globally Normalized Transition-Based Neural Networks}},
url = {http://arxiv.org/abs/1603.06042 http://arxiv.org/abs/1603.06042v1$\backslash$npapers3://publication/uuid/7FBA32B7-E853-4E1A-96D4-5AF9461236EC},
volume = {cs.CL},
year = {2016}
}
@book{AssociationforComputationalLinguistics.Meeting45th:2007:Prague2007,
abstract = {v. [1] Main volume (i.e. conference proceedings) -- v. [2] Companion volume: proceedings of the Student Research Workshop; proceedings of demo and poster sessions; tutorial abstracts.},
author = {{Association for Computational Linguistics. Meeting (45th : 2007 : Prague}, Czech Republic) and {Association for Computational Linguistics.}, Roger},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Association for Computational Linguistics. Meeting (45th 2007 Prague, Association for Computational Linguistics. - 2007 - ACL 2007 pr.pdf:pdf},
isbn = {9781932432879},
pages = {934--944},
publisher = {Association for Computational Linguistics},
title = {{ACL 2007 : proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic.}},
year = {2007}
}
@inproceedings{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {ICLR},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
isbn = {0147-006X (Print)},
issn = {0147-006X},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473 http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@article{Belanger2015,
abstract = {We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of the outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques.},
archivePrefix = {arXiv},
arxivId = {1511.06350},
author = {Belanger, David and McCallum, Andrew},
eprint = {1511.06350},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belanger, McCallum - 2015 - Structured Prediction Energy Networks.pdf:pdf},
journal = {ICML},
pages = {1--15},
title = {{Structured Prediction Energy Networks}},
url = {http://arxiv.org/abs/1511.06350},
volume = {48},
year = {2016}
}
@article{Bengio2015,
abstract = {The phase diagram of electron-doped pnictides is studied varying the temperature, electronic density, and isotropic quenched disorder strength by means of computational techniques applied to a three-orbital ({\$}xz{\$}, {\$}yz{\$}, {\$}xy{\$}) spin-fermion model with lattice degrees of freedom. In experiments, chemical doping introduces disorder but in theoretical studies the relationship between electronic doping and the randomly located dopants, with their associated quenched disorder, is difficult to address. In this publication, the use of computational techniques allows us to study independently the effects of electronic doping, regulated by a global chemical potential, and impurity disorder at randomly selected sites. Surprisingly, our Monte Carlo simulations reveal that the fast reduction with doping of the N$\backslash$'eel {\$}T{\_}N{\$} and the structural {\$}T{\_}S{\$} transition temperatures, and the concomitant stabilization of a robust nematic state, is primarily controlled by the magnetic dilution associated with the in-plane isotropic disorder introduced by Fe substitution. In the doping range studied, changes in the Fermi Surface produced by electron doping affect only slightly both critical temperatures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03099v2},
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
doi = {10.1201/9781420049176},
eprint = {arXiv:1506.03099v2},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - Unknown - Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:pdf},
isbn = {0849371813},
issn = {1941-6016},
journal = {NIPS},
pages = {1--9},
title = {{Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks}},
year = {2015}
}
@inproceedings{Bertasius,
abstract = {Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as tex- ture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional ap- proaches, we show that we can invert the commonly estab- lished pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object- related features as high-level cues for contour detection. We achieve this goal by means of a multi-scale deep net- work that consists of five convolutional layers and a bifur- cated fully-connected sub-network. The section from the in- put layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large- scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently- trained branches. One branch learns to predict the con- tour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human la- belers agreeing about the contour presence at a given point (with a regression criterion). We show that without any feature engineering our multi- scale deep learning approach achieves state-of-the-art re- sults in contour detection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1123v3},
author = {Bertasius, Gedas and Shi, Jianbo and Torresani, Lorenzo},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299067},
eprint = {arXiv:1412.1123v3},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertasius, Shi, Torresani - Unknown - DeepEdge A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {4380--4389},
title = {{DeepEdge: A multi-scale bifurcated deep network for top-down contour detection}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{DBLP:conf/icml/Boulanger-LewandowskiBV12,
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, {\{}ICML{\}} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
publisher = {icml.cc / Omnipress},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
url = {http://icml.cc/discuss/2012/590.html},
year = {2012}
}
@article{Bowman2015,
abstract = {The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
eprint = {1511.06349},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
journal = {ICLR},
month = {nov},
pages = {1--13},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2016}
}
@article{Chen2015,
abstract = {In recent years the performance of deep learning algorithms has been demonstrated in a variety of application domains. The goal of this paper is to enrich deep learning to be able to predict a set of random variables while taking into account their dependencies. Towards this goal, we propose an efficient algorithm that is able to learn structured models with non-linear functions. We demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from noisy images, and show that by jointly learning multilayer perceptrons and pairwise features, significant gains in performance can be obtained.},
archivePrefix = {arXiv},
arxivId = {1407.2538},
author = {Chen, Liang-Chieh and Schwing, Alexander G and Yuille, Alan L and Urtasun, Raquel},
eprint = {1407.2538},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2015 - Learning Deep Structured Models.pdf:pdf},
isbn = {9781510810587},
journal = {Journal of Machine Learning Research},
pages = {10},
title = {{Learning Deep Structured Models}},
url = {http://arxiv.org/abs/1407.2538},
volume = {37},
year = {2015}
}
@article{Chen2015a,
abstract = {In recent years the performance of deep learning algorithms has been demonstrated in a variety of application domains. The goal of this paper is to enrich deep learning to be able to predict a set of random variables while taking into account their dependencies. Towards this goal, we propose an efficient algorithm that is able to learn structured models with non-linear functions. We demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from noisy images, and show that by jointly learning multilayer perceptrons and pairwise features, significant gains in performance can be obtained.},
archivePrefix = {arXiv},
arxivId = {1407.2538},
author = {Chen, Liang-Chieh and Schwing, Alexander G and Yuille, Alan L and Urtasun, Raquel},
eprint = {1407.2538},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2015 - Learning Deep Structured Models(2).pdf:pdf},
isbn = {9781510810587},
journal = {Journal of Machine Learning Research},
pages = {10},
title = {{Learning Deep Structured Models}},
url = {http://arxiv.org/abs/1407.2538},
volume = {37},
year = {2015}
}
@inproceedings{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
eprint = {1506.02216},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
month = {jun},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@article{Cirean,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efﬁciently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artiﬁcial neural network as a pixel classiﬁer. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classiﬁer is trained by plain gradient descent on a 512 512 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-speciﬁc postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
author = {Ciresan, Dc and Giusti, Alessandro and Gambardella, Lm and Schmidhuber, J},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cirean, Giusti, Gambardella - Unknown - Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {NIPS},
pages = {1--9},
title = {{Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images}},
url = {https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
year = {2012}
}
@article{Clark2016,
abstract = {A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning to search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1606.01323},
author = {Clark, Kevin and Manning, Christopher D},
eprint = {1606.01323},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clark, Manning - Unknown - Improving Coreference Resolution by Learning Entity-Level Distributed Representations.pdf:pdf},
journal = {ACL},
title = {{Improving Coreference Resolution by Learning Entity-Level Distributed Representations}},
url = {http://arxiv.org/abs/1606.01323},
year = {2016}
}
@inproceedings{cohn2008sentence,
author = {Cohn, Trevor and Lapata, Mirella},
booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1},
organization = {Association for Computational Linguistics},
pages = {137--144},
title = {{Sentence compression beyond word deletion}},
year = {2008}
}
@article{Costa-Jussa2016,
abstract = {Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.},
archivePrefix = {arXiv},
arxivId = {1603.00810},
author = {Costa-Juss{\`{a}}, Marta R. and Fonollosa, Jos{\'{e}} A. R.},
eprint = {1603.00810},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Costa-Juss{\`{a}}, Fonollosa - 2016 - Character-based Neural Machine Translation.pdf:pdf},
journal = {ACL},
month = {mar},
title = {{Character-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1603.00810},
year = {2016}
}
@inproceedings{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, Andrew M. and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems (NIPS '15)},
eprint = {1511.01432},
pages = {1--9},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@article{Daudaravicius2016,
abstract = {The Automated Evaluation of Scientific Writ-ing, or AESW, is the task of identifying sen-tences in need of correction to ensure their ap-propriateness in a scientific prose. The data set comes from a professional editing company, VTeX, with two aligned versions of the same text – before and after editing – and covers a variety of textual infelicities that proofreaders have edited. While previous shared tasks fo-cused solely on grammatical errors (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014), this time edits cover other types of linguistic misfits as well, in-cluding those that almost certainly could be interpreted as style issues and similar " matters of opinion " . The latter arise because of dif-ferent language editing traditions, experience, and the absence of uniform agreement on what " good " scientific language should look like. Initiating this task, we expected the participat-ing teams to help identify the characteristics of " good " scientific language, and help create a consensus of which language improvements are acceptable (or necessary). Six participat-ing teams took on the challenge.},
author = {Daudaravicius, Vidas and Banchs, Rafael E and Volodina, Elena and Napoles, Courtney},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daudaravicius et al. - 2016 - A Report on the Automatic Evaluation of Scientific Writing Shared Task.pdf:pdf},
journal = {NAACL BEA11 Workshop},
pages = {53--62},
title = {{A Report on the Automatic Evaluation of Scientific Writing Shared Task}},
year = {2016}
}
@inproceedings{daume05learning,
author = {{Daum{\'{e}} III}, Hal and Marcu, Daniel},
booktitle = {Proceedings of the Twenty-Second International Conference on Machine Learning {\{}(ICML{\}} 2005)},
pages = {169--176},
title = {{Learning as search optimization: approximate large margin methods for structured prediction}},
year = {2005}
}
@book{Diamond,
abstract = {EDUCATION Designing and Assessing Courses and Curricula refl ects the most current knowledge and practice in course and curriculum design and connects this knowledge with the critical task of assessing learning outcomes at both course and curricular levels. This thoroughly revised and expanded third edition of the best-selling book positions course design as a tool for educational change and contains a wealth of new material including new chapters, case examples, and resources. This edition covers timely topics such as • The growing role of faculty in accreditation • Scholarship and faculty rewards • Teaching and learning research • Meeting the needs of adult learners • Addressing the challenges of distance learning " From pioneer to seasoned scholar of instructional development, Diamond began with the practical and moved to the transformative. What makes this latest edition especially compelling is its consideration of the designer as change agent within the current turbulent environment of higher education. Recognizing that the best-laid instructional plans are doomed to failure if they aren't aligned with organizational conditions, Diamond wisely situates his cogent treatment of design issues within the context of change, adding a critical dimension that is often neglected. " —Nancy Van Note Chism, professor of educational leadership and policy studies, Indiana University School of Education, IUPUI " This book dives into the most important challenge we face in the academy: helping today's underprepared students reap the full benefi ts of their time in college. Diamond knows that what matters in college is not the information imparted, but rather the higher order capabilities that students do—or do not—attain through their studies. Designed for everyone who wants to know 'what works?', this magnifi cently useful handbook draws together a generation of research and campus experimentation on what it will take to create a more intentional, inclusive, and educationally effective college curriculum. Published shortly after Robert Diamond's untimely death, Designing and Assessing Courses and Curricula is his lasting gift to the entire higher education community. " —Carol Geary Schneider, president, the Association of American Colleges and Universities},
author = {Diamond, Robert M},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Diamond - Unknown - Designing and Assessing Courses and Curricula.pdf:pdf},
isbn = {0-7879-1030-9},
pages = {27--84},
pmid = {15245412},
title = {{Designing and assessing courses and curricula}},
url = {www.josseybass.com},
year = {2008}
}
@article{Domke2013,
abstract = {—Likelihood based-learning of graphical models faces chal-lenges of computational-complexity and robustness to model mis-specification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learn-ing performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3193v1},
author = {Domke, Justin},
eprint = {arXiv:1301.3193v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domke - 2013 - Learning Graphical Model Parameters with Approximate Marginal Inference.pdf:pdf},
keywords = {()},
title = {{Learning Graphical Model Parameters with Approximate Marginal Inference}},
year = {2013}
}
@incollection{Domke,
abstract = {In a setting where approximate inference is necessary, structured learning can be formulated as a joint optimization of inference " messages " and local potentials. This chapter observes that, for fixed messages, the optimization problem with respect to potentials takes the form of a logistic regression problem, biased by the current set of messages. This observation leads to an algorithm that alternates between message-passing inference updates, and learning updates. It is possible to employ any set of potential functions where an algorithm exists to optimize a logistic loss, including linear functions, boosted decision trees, and multi-layer perceptrons.},
author = {Domke, Justin},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domke - Unknown - Training Structured Predictors Through Iterated Logistic Regression.pdf:pdf},
pages = {1--26},
title = {{Training Structured Predictors Through Iterated Logistic Regression}}
}
@inproceedings{dorr2003hedge,
author = {Dorr, Bonnie and Zajic, David and Schwartz, Richard},
booktitle = {Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5},
organization = {Association for Computational Linguistics},
pages = {1--8},
title = {{Hedge trimmer: A parse-and-trim approach to headline generation}},
year = {2003}
}
@article{elman1990finding,
author = {Elman, Jeffrey L},
doi = {10.1207/s15516709cog1402_1},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {179--211},
pmid = {19563812},
publisher = {Elsevier},
title = {{Finding structure in time}},
volume = {14},
year = {1990}
}
@article{Felice2014,
author = {Felice, M and Yuan, Z},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Felice, Yuan - 2014 - Generating artificial errors for grammatical error correction.pdf:pdf},
journal = {EACL},
title = {{Generating artificial errors for grammatical error correction.}},
url = {http://www.aclweb.org/old{\_}anthology/E/E14/E14-3.pdf{\#}page=126},
year = {2014}
}
@article{Ganin,
abstract = {We propose a new architecture for difficult image processing operations, such as natural edge detection or thin object segmentation. The architecture is based on a simple combination of convolutional neural networks with the nearest neighbor search. We focus our attention on the situations when the desired image transformation is too hard for a neural network to learn explicitly. We show that in such situations, the use of the nearest neighbor search on top of the network output allows to im-prove the results considerably and to account for the underfitting effect during the neural network training. The approach is validated on three challenging bench-marks, where the performance of the proposed architecture matches or exceeds the state-of-the-art.},
author = {Ganin, Yaroslav and Lempitsky, Victor},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin, Lempitsky - Unknown - N 4 -Fields Neural Network Nearest Neighbor Fields for Image Transforms.pdf:pdf},
title = {{N 4 -Fields: Neural Network Nearest Neighbor Fields for Image Transforms}}
}
@article{gers2001lstm,
abstract = {Previous work on learning regular languages from exemplary training sequences showed that Long ShortTerm Memory (LSTM) outperforms traditional recurrent neural networks (RNNs). Here we demonstrate LSTM{\&}{\#}039;s superior performance on context free language (CFL) benchmarks for recurrent neural networks (RNNs), and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the rst RNNs to learn a simple context sensitive language (CSL), namely a n b n c n . Index Terms: Recurrent neural networks, long short-term memory, context free languages, context sensitive languages. 1 Introduction Recurrent neural networks (RNNs) are remarkably general sequence processing devices (Siegelmann {\&} Sontag, 1991). In principle they are applicable to tasks beyond the reach of Hidden Markov Models (HMMs) or discrete symbolic grammar learning algorithms (SGLAs) (Lee, 1996; Sakakibara, 1997). For instance, unlike RNNs, ...},
author = {Gers, Felix A and Schmidhuber, J{\"{u}}rgen},
journal = {IEEE Transactions on Neural Networks},
keywords = {formal-language,lstm,rnn},
number = {6},
pages = {1333--1340},
publisher = {IEEE},
title = {{LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.3536$\backslash$n/home/kermorvant/refbase{\_}files/2001/gers/1046{\_}gers{\_}schmidhuber{\_}felix2001.pdf},
volume = {12},
year = {2001}
}
@article{graves13generating,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
volume = {abs/1308.0},
year = {2013}
}
@misc{notes2002,
author = {Grinstein, Georges and Keim, Daniel and Ward, Matthew},
howpublished = {IEEE Visualization Course {\#}1 Notes},
month = {oct},
title = {{Information Visualization, Visual Data Mining, and Its Application to Drug Design}},
url = {http://vis.computer.org/vis2002/program/tutorials/tutorial{\_}01{\_}abstract.html},
year = {2002}
}
@article{hochreiter1997lstm,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit},
volume = {9},
year = {1997}
}
@inproceedings{Jain2007,
author = {Jain, Viren and Murray, Joseph F. and Roth, Fabian and Turaga, Srinivas and Zhigulin, Valentin and Briggman, Kevin L. and Helmstaedter, Moritz N. and Denk, Winfried and Seung, H. Sebastian},
booktitle = {2007 IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408909},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain et al. - 2007 - Supervised Learning of Image Restoration with Convolutional Networks.pdf:pdf},
isbn = {978-1-4244-1630-1},
keywords = {Bayesian learning,Circuit noise,Degradation,Electron microscopy,Humans,Image processing,Image restoration,Markov processes,Markov random field,Markov random fields,Object recognition,Supervised learning,Training data,anisotropic diffusion algorithms,computer vision,conditional random field,convolutional networks,degraded images,electron microscopic images,gradient learning,high-level vision problems,image restoration,inference mechanisms,inference procedures,learning (artificial intelligence),low-level image processing,neural circuitry,object recognition,supervised learning},
pages = {1--8},
publisher = {IEEE},
title = {{Supervised Learning of Image Restoration with Convolutional Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408909},
year = {2007}
}
@inproceedings{Ji2016,
abstract = {This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations that link adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. By marginalizing over latent discourse relations, it also yields a language model that improves on a strong recurrent neural network baseline.},
archivePrefix = {arXiv},
arxivId = {1603.01913},
author = {Ji, Yangfeng and Haffari, Gholamreza and Eisenstein, Jacob},
booktitle = {NAACL},
eprint = {1603.01913},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Haffari, Eisenstein - 2016 - A Latent Variable Recurrent Neural Network for Discourse Relation Language Models.pdf:pdf},
month = {mar},
pages = {to appear},
title = {{A Latent Variable Recurrent Neural Network for Discourse Relation Language Models}},
url = {http://arxiv.org/abs/1603.01913},
year = {2016}
}
@article{jing2002using,
abstract = {Professional summarizers often reuse original documents to generate summaries.The task of summary sentence decomposition is to deduce whether a summary sentence is constructed by reusing the original text and to identify reused phrases. Specifically, the decomposition program needs to answer three questions for a given summary sentence: (1) Is this summary sentence constructed by reusing the text in the original document? (2) If so, what phrases in the sentence come from the original document? and (3) From where in the document do the phrases come? Solving the decomposition problem can lead to better text generation techniques for summarization. Decomposition can also provide large training and testing corpora for extraction-based summarizers. We propose a hidden Markov model solution to the decomposition problem. Evaluations show that the proposed algorithm performs well.},
author = {Jing, Hongyan},
doi = {10.1162/089120102762671972},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {4},
pages = {527--543},
publisher = {MIT Press},
title = {{Using Hidden Markov Modeling to Decompose Human-Written Summaries}},
volume = {28},
year = {2002}
}
@inproceedings{Jozefowicz2016,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
archivePrefix = {arXiv},
arxivId = {1602.02410},
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
booktitle = {Arxiv},
eprint = {1602.02410},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jozefowicz et al. - Unknown - Exploring the Limits of Language Modeling.pdf:pdf},
title = {{Exploring the Limits of Language Modeling}},
url = {http://arxiv.org/abs/1602.02410},
year = {2016}
}
@article{kadar2015lingusitic,
author = {K{\'{a}}d{\'{a}}r, Akos and Chrupa{\l}a, Grzegorz and Alishahi, Afra},
title = {{Lingusitic Analysis of multi-modal Recurrent Neural Networks}},
year = {2015}
}
@article{kadar2016representation,
author = {K{\'{a}}d{\'{a}}r, {\'{A}}kos and Chrupa{\l}a, Grzegorz and Alishahi, Afra},
journal = {arXiv preprint arXiv:1602.08952},
title = {{Representation of linguistic form and function in recurrent neural networks}},
year = {2016}
}
@inproceedings{Kalchbrenner2013,
abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {\textgreater} 43{\%} lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and mean- ing of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Kalchbrenner, Nal and Blunsom, Phil},
booktitle = {Emnlp},
eprint = {1409.0473},
isbn = {9781937284978},
number = {October},
pages = {1700--1709},
title = {{Recurrent Continuous Translation Models}},
year = {2013}
}
@article{karpathy2015visualizing,
author = {Karpathy, Andrej and Johnson, Justin and Li, Fei-Fei},
journal = {ICLR Workshops},
title = {{Visualizing and understanding recurrent networks}},
year = {2015}
}
@inproceedings{Kim2014,
author = {Kim, Yoon},
booktitle = {Proceedings of EMNLP},
title = {{Convolutional Neural Networks for Sentence Classification}},
year = {2014}
}
@article{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
eprint = {1508.06615},
journal = {AAAI},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {ICLR},
eprint = {1312.6114},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2014 - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
month = {dec},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@article{Kingma,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5298v1},
author = {Kingma, Dp and Rezende, Dj and Welling, Max},
eprint = {arXiv:1406.5298v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma et al. - Unknown - Semi-supervised Learning with Deep Generative Models.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{Semi-supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@article{Kivinen,
abstract = {This paper investigates visual boundary de-tection, i.e. prediction of the presence of a boundary at a given image location. We de-velop a novel neurally-inspired deep architec-ture for the task. Notable aspects of our work are (i) the use of " covariance features " [Ranzato and Hinton, 2010] which depend on the squared response of a filter to the in-put image, and (ii) the integration of im-age information from multiple scales and se-mantic levels via multiple streams of inter-linked, layered, and non-linear " deep " pro-cessing. Our results on the Berkeley Segmen-tation Data Set 500 (BSDS500) show com-parable or better performance to the top-performing methods [Arbelaez et al., 2011, Ren and Bo, 2012, Lim et al., 2013, Doll{\'{a}}r and Zitnick, 2013] with effective inference times. We also propose novel quantitative assessment techniques for improved method understanding and comparison. We care-fully dissect the performance of our architec-ture, feature-types used and training meth-ods, providing clear signals for model under-standing and development.},
author = {Kivinen, Jyri J and Williams, Christopher K I and Heess, Nicolas and Technologies, DeepMind},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kivinen, Williams, Heess - Unknown - Visual Boundary Prediction A Deep Neural Prediction Network and Quality Dissection.pdf:pdf},
issn = {15337928},
journal = {Proc. {\{}AISTATS{\}}},
pages = {512--521},
title = {{Visual boundary prediction: {\{}A{\}} deep neural prediction network and quality dissection}},
volume = {33},
year = {2014}
}
@article{Koo2010,
abstract = {This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98{\%} of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.},
author = {Koo, Terry and Rush, Alexander M. and Collins, Michael and Jaakkola, Tommi and Sontag, David},
journal = {EMNLP},
keywords = {Dependency Parsing,Dual Decomposition,Non-Projective},
number = {October},
pages = {1288--1298},
title = {{Dual Decomposition for Parsing with Non-Projective Head Automata}},
year = {2010}
}
@inproceedings{krause2016interacting,
abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough esti-mate condensed into one number. However, our research goes beyond these nave estimates through the design and imple-mentation of an interactive visual analytics system, Prospec-tor. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the predic-tion overall. In addition, our support for localized inspection allows data scientists to understand how and why specific dat-apoints are predicted as they are, as well as support for tweak-ing feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for de-tecting the onset of diabetes from electronic medical records.},
author = {Krause, Josua and Perer, Adam and Ng, Kenney},
booktitle = {ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/2858036.2858529},
isbn = {9781450333627},
keywords = {Author Keywords interactive machine learning,Miscellaneous,partial dependence,predictive modeling,visual analytics},
organization = {ACM},
pages = {5686--5697},
title = {{Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models}},
year = {2016}
}
@article{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and {Geoffrey E.}, Hinton},
eprint = {1102.0183},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 25 (NIPS2012)},
pages = {1--9},
title = {{Imagenet}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@inproceedings{Lecun2012,
abstract = {Fast visual recognition in the mammalian cortex seems to be a hier- archical process by which the representation of the visual world is transformed in multiple stages from low-level retinotopic features to high-level, global and invariant features, and to object categories. Every single step in this hierarchy seems to be subject to learning. How does the visual cortex learn such hierar- chical representations by just looking at the world? How could computers learn such representations from data? Computer vision models that areweakly inspired by the visual cortex will be described. A number of unsupervised learning algo- rithms to train these models will be presented, which are based on the sparse auto-encoder concept. The effectiveness of these algorithms for learning invari- ant feature hierarchies will be demonstrated with a number of practical tasks such as scene parsing, pedestrian detection, and object classification. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1091v1},
author = {LeCun, Yann},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33863-2_51},
eprint = {arXiv:1411.1091v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun - 2012 - LNCS 7583 - Learning Invariant Feature Hierarchies.pdf:pdf},
isbn = {9783642338625},
issn = {03029743},
number = {PART 1},
pages = {496--505},
pmid = {1000102572},
title = {{Learning invariant feature hierarchies}},
volume = {7583 LNCS},
year = {2012}
}
@inproceedings{Lee,
abstract = {A computer conversational system can potentially help a foreign- language$\backslash$nstudent improve his/her fluency through practice dia- logues. One$\backslash$nof its potential roles could be to correct ungrammat- ical sentences.$\backslash$nThis paper1 describes our research on a sentence- level, generation-based$\backslash$napproach to grammar correction: first, a word lattice of candidate$\backslash$ncorrections is generated from an ill- formed input. A traditional$\backslash$nn-gram language model is used to pro- duce a small set of N-best$\backslash$ncandidates, which are then reranked by parsing using a stochastic$\backslash$ncontext-free grammar. We evaluate this approach in a flight domain$\backslash$nwith simulated ill-formed sentences. We discuss its potential applications$\backslash$nin a few related tasks.},
author = {Lee, John and Seneff, Stephanie},
booktitle = {INTERSPEECH 2006- ICSLP Automatic Grammar Correction for Second-Language Learners},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seneff - Unknown - Automatic Grammar Correction for Second-Language Learners.pdf:pdf},
isbn = {9781604234497},
keywords = {[Electronic Manuscript],grade{\_}a},
pages = {1978--1981},
title = {{Automatic Grammar Correction for Second-Language Learners}},
year = {2006}
}
@inproceedings{li2015visualizing,
abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
address = {San Diego, California},
archivePrefix = {arXiv},
arxivId = {1506.01066},
author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
booktitle = {NAACL},
eprint = {1506.01066},
keywords = {Neural Network,Visualization},
month = {jun},
pages = {1--10},
publisher = {Association for Computational Linguistics},
title = {{Visualizing and Understanding Neural Models in NLP}},
url = {http://www.aclweb.org/anthology/N16-1082 http://arxiv.org/abs/1506.01066},
year = {2016}
}
@article{Lin,
abstract = {How should we teach machine learning to a diverse audience? Is a 'foundation-based' ap-proach appropriate or should we switch to a 'techniques-oriented' approach? We argue that the foundation-based approach is still the way to go. The foundation-based approach emphasizes the fundamentals first, before moving to al-gorithms or paradigms. Here, we focus on three key concepts in machine learning that cover the basic understanding, the al-gorithmic modeling, and the practical tun-ing. We then demonstrate how more sophis-ticated techniques like the popular support vector machine can be understood within this framework.},
author = {Lin, Hsuan-Tien and Madgon-Ismail, Malik and Abu-Mostafa, Yaser S},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Madgon-Ismail, Abu-Mostafa - Unknown - Teaching Machine Learning to a Diverse Audience the Foundation-based Approach.pdf:pdf},
title = {{Teaching Machine Learning to a Diverse Audience: the Foundation-based Approach}}
}
@inproceedings{luong15effective,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
booktitle = {EMNLP},
eprint = {1508.04025},
isbn = {9781941643327},
number = {September},
pages = {11},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1508.04025},
year = {2015}
}
@article{Manning,
abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
author = {Manning, Christopher D},
doi = {10.1162/COLI},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manning - Unknown - Last Words Computational Linguistics and Deep Learning(2).pdf:pdf},
isbn = {9781608459858},
issn = {04194217},
journal = {Computational Linguistics},
number = {4},
pages = {701--707},
title = {{Computational Linguistics and Deep Learning}},
volume = {41},
year = {2015}
}
@article{Martin2000,
author = {Martin, JH and Jurafsky, D},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Jurafsky - 2000 - Speech and language processing.pdf:pdf},
journal = {International Edition},
title = {{Speech and language processing}},
url = {http://www.ulb.tu-darmstadt.de/tocs/203636384.pdf},
year = {2000}
}
@article{Mikolov2013a,
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
journal = {arXiv},
title = {{Efficient estimation of word representations in vector space}},
year = {2013}
}
@article{Moritz,
abstract = {Teaching machines to read natural language documents remains an elusive chal-lenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03340v1},
author = {Hermann, Karm Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
eprint = {arXiv:1506.03340v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moritz et al. - Unknown - Teaching Machines to Read and Comprehend.pdf:pdf},
issn = {10495258},
journal = {NIPS},
pages = {1--13},
title = {{Teaching Machines to Read and Comprehend}},
year = {2015}
}
@article{Park2011,
abstract = {Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased perfor-mance. Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the correc-tions to a limited scope. We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the pa-rameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the bur-den of acquiring a large corpora of corrected sentences. We also present a cheap and effi-cient way to provide automated evaluation re-sults for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.},
author = {Park, Y Albert and Levy, Roger},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Association for Computational Linguistics. Meeting (45th 2007 Prague, Association for Computational Linguistics. - 2007 - ACL 2007 pr.pdf:pdf},
pages = {934--944},
title = {{Automated Whole Sentence Grammar Correction Using a Noisy Channel Model}},
year = {2011}
}
@inproceedings{Pinto2011,
author = {Pinto, Nicolas and Barhomi, Youssef and Cox, David D. and DiCarlo, James J.},
booktitle = {2011 IEEE Workshop on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2011.5711540},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinto et al. - 2011 - Comparing state-of-the-art visual features on invariant object recognition tasks.pdf:pdf},
isbn = {978-1-4244-9496-5},
keywords = {Face,Kernel,Object recognition,Pixel,Testing,Training,Visualization,baseline representation,computer vision,identity-preserving image variation,image representation,invariance problem,invariant object recognition task,natural image benchmark,object recognition,standard natural image database benchmark,synthetic recognition task,visual databases,visual object recognition system,visual representation},
month = {jan},
pages = {463--470},
publisher = {IEEE},
title = {{Comparing state-of-the-art visual features on invariant object recognition tasks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5711540},
year = {2011}
}
@article{Ranzato2016,
abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.},
archivePrefix = {arXiv},
arxivId = {1511.06732},
author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
eprint = {1511.06732},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ranzato et al. - 2016 - Sequence Level Training with Recurrent Neural Networks.pdf:pdf},
journal = {ICLR},
keywords = {Optimization,RNN},
month = {nov},
pages = {1--15},
title = {{Sequence Level Training with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06732},
year = {2016}
}
@article{ranzato16sequence,
abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.},
archivePrefix = {arXiv},
arxivId = {1511.06732},
author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
eprint = {1511.06732},
journal = {ICLR},
keywords = {Optimization,RNN},
pages = {1--15},
title = {{Sequence Level Training with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06732},
year = {2016}
}
@inproceedings{Razavian,
abstract = {Recent results indicate that the generic descriptors ex- tracted from the convolutional neural networks are very powerful [ 10 , 29 , 48 ]. This paper adds to the mount- ing evidence that this is indeed the case. We report on a series of experiments conducted for different recogni- tion tasks using the publicly available code and model of the OverFeat network which was trained to perform ob- ject classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image represen- tation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the orig- inal task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L 2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representa- tions are further modified using simple augmentation tech- niques e.g. jittering. The results strongly suggest that fea- tures obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Razavian et al. - Unknown - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
pages = {512--519},
title = {{CNN features off-the-shelf: An astounding baseline for recognition}},
year = {2014}
}
@inproceedings{wiles1998recurrent,
author = {Rodriguez, Paul and Wiles, Janet},
booktitle = {Advances in Neural Information Processing Systems},
isbn = {0262100762},
issn = {10495258},
keywords = {juergen},
organization = {MIT Press},
pages = {87--93},
title = {{Recurrent Neural Networks Can Learn to Implement Symbol-Sensitive Counting}},
volume = {10},
year = {1998}
}
@article{Rubin,
abstract = {Most statistics educators would agree that statistical inference is both the central objective of statistical reasoning and one of the most difficult ideas for students to understand. In traditional approaches, statistical inference is introduced as a quantitative problem, usually of figuring out the probability of obtaining an observed result on the assumption that the null hypothesis is true. In this article, we lay out an alternative approach towards teaching statistical inference that we are calling “informal inference.” We begin by describing informal inference and then illustrate ways we have been trying to develop the component ideas of informal inference in a recent data analysis seminar with teachers; our particular emphasis in this article is on the ways in which teachers used TinkerPlots, a statistical visualization tool. After describing teachers' approaches to an inferential task, we offer some preliminary hypotheses about the conceptual issues that arose for them},
author = {Rubin, Andee and Hammerman, J and Konold, Cliff},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin, Hammerman - Unknown - EXPLORING INFORMAL INFERENCE WITH INTERACTIVE VISUALIZATION SOFTWARE.pdf:pdf},
journal = {Proceedings of the Sixth {\ldots}},
pages = {1--6},
title = {{Exploring informal inference with interactive visualization software}},
url = {http://www.researchgate.net/publication/228353456{\_}Exploring{\_}informal{\_}inference{\_}with{\_}interactive{\_}visualization{\_}software/file/60b7d52262d93072e8.pdf},
year = {2006}
}
@article{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
eprint = {1509.00685},
isbn = {9781941643327},
journal = {In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {September},
pages = {379--389},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@article{Rush2011,
abstract = {We describe an exact decoding algorithm for syntax-based statistical translation. The ap- proach uses Lagrangian relaxation to decom- pose the decoding problem into tractable sub- problems, thereby avoiding exhaustive dy- namic programming. Themethod recovers ex- act solutions, with certificates of optimality, on over 97{\%} of test examples; it has compa- rable speed to state-of-the-art decoders.},
author = {Rush, Alexander M and Collins, Michael},
journal = {Computational Linguistics},
pages = {72--82},
title = {{Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation}},
url = {http://www.aclweb.org/anthology/P11-1008},
year = {2011}
}
@article{Rush,
abstract = {Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for de-pendency parsing using linear-time vine prun-ing and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their un-pruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an un-pruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.},
author = {Rush, Alexander M and Petrov, Slav},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rush, Petrov - Unknown - Vine Pruning for Efficient Multi-Pass Dependency Parsing.pdf:pdf},
title = {{Vine Pruning for Efficient Multi-Pass Dependency Parsing}}
}
@article{Rush2010,
abstract = {This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.},
author = {Rush, Alexander M. and Sontag, David and Collins, Michael and Jaakkola, Tommi},
journal = {EMNLP},
keywords = {Dual Decomposition,Linear Programming},
number = {October},
pages = {1--11},
title = {{On dual decomposition and linear programming relaxations for natural language processing}},
url = {http://dl.acm.org/citation.cfm?id=1870658.1870659},
year = {2010}
}
@article{Russell2006,
abstract = {An introductory Artificial Intelligence (AI) course provides students with basic knowledge of the theory and practice of AI as a discipline concerned with the methodology and technology for solving problems that are difficult to solve by other means. It is generally recognized that an introductory Artificial Intelligence course is challenging to teach. This is, in part, due to the diverse and seemingly disconnected core AI topics that are typically covered. Recently, work has been done to address the diversity of topics covered in the course and to create a theme-based approach. Russell and Norvig present an agent-centered approach [9]. Others have been working to integrate Robotics into the AI course [1, 2, 3]. We present work on a project funded by the National Science Foundation with a goal of unifying the artificial intelligence course around the theme of machine learning. This involves the development and testing of an adaptable framework for the presentation of core AI topics that emphasizes the relationship be-tween AI and computer science. Machine learning is inherently connected with the AI core topics and provides methodology and technology to enhance real-world applications within many of these topics. Machine learning also provides a bridge between AI technology and modern software engineering. In his article, Mitchell discusses the increasingly important role that machine learning plays in the software world and identifies three important areas: data mining, difficult-to-program applications, and customized software applications [6]. We have developed a suite of adaptable, hands-on laboratory pro-jects that can be closely integrated into the introductory AI course. Each project involves the design and implementation of a learning system which will enhance a particular commonly-deployed application. The goal is to enhance the student learning experience in the introductory artificial intelligence course by (1) introducing machine learning elements into the AI course, (2) implementing a set of unifying machine learning laboratory projects to tie together the core AI topics, and (3) developing, applying, and testing an adaptable framework for the presentation of core AI topics which emphasizes the important relationship between AI and computer science in general, and software development in particular. Details on this project as well as samples of course materials developed are published in [4, 5, 7, 8] and are available at the project website at We present an overview of our work along with a detailed presentation of one of these projects and how it meets our goals. The project involves the development of a learning system for web document classification. Students investigate the process of classifying hypertext documents, called tagging, and apply machine learning techniques and data mining tools for automatic tagging. Our experiences using the projects are also presented.},
author = {Russell, Ingrid and Markov, Zdravko and Neller, Todd},
doi = {10.1145/1140123.1140230},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russell, Markov, Neller - Unknown - Teaching AI through Machine Learning Projects.pdf:pdf},
isbn = {1595936033},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {Artificial Intelligence,Computer Science Education General Terms,Experimentation Keywords,Projects},
number = {3},
pages = {323},
title = {{Teaching AI through machine learning projects}},
url = {http://uhaweb.hartford.edu/compsci/ccli.},
volume = {38},
year = {2006}
}
@unpublished{Schmaltz2016,
abstract = {We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.},
archivePrefix = {arXiv},
arxivId = {1604.04677},
author = {Schmaltz, Allen and Kim, Yoon and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {arxiv},
eprint = {1604.04677},
title = {{Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction}},
url = {http://arxiv.org/abs/1604.04677},
year = {2016}
}
@unpublished{Schmaltz2016a,
abstract = {We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.},
archivePrefix = {arXiv},
arxivId = {1604.04677},
author = {Schmaltz, Allen and Kim, Yoon and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {arxiv},
eprint = {1604.04677},
title = {{Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction}},
url = {http://arxiv.org/abs/1604.04677},
year = {2016}
}
@article{Schulman2015,
abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
archivePrefix = {arXiv},
arxivId = {1506.05254},
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
eprint = {1506.05254},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Gradient Estimation Using Stochastic Computation Graphs.pdf:pdf},
issn = {10495258},
journal = {NIPS},
month = {jun},
pages = {1--13},
title = {{Gradient Estimation Using Stochastic Computation Graphs}},
url = {http://arxiv.org/abs/1506.05254},
year = {2015}
}
@article{Schweitzer,
abstract = {Engaging students in the learning process has been shown to be an effective means for education. Several methods have been proposed to achieve this engagement for computer science and other disciplines. Active learning is one such technique that incorporates interactive classroom activities to reinforce concepts and involve the students. Visualizations of computer science concepts such as algorithm animations can be used for these activities. To be most effective in this environment, they need to be designed and used with active learning in mind. This paper describes the design characteristics of such visualizations, ways of using them in the classroom, and our experience with developing and using visualization tools across different courses in the computer science curriculum.},
author = {Schweitzer, Dino and Brown, Wayne},
doi = {10.1145/1227504.1227384},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schweitzer, Brown - 2007 - Interactive visualization for the active learning classroom.pdf:pdf},
isbn = {1595933611},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {Collaborative Learning,Computer Science education General Terms Design,Experimentation,Human Factors Keywords Active Learning,Visualization},
number = {1},
pages = {208},
title = {{Interactive visualization for the active learning classroom}},
volume = {39},
year = {2007}
}
@article{Shaffer2010,
author = {Shaffer, Clifforda and Cooper, Matthew L. and Alon, Alexander Joel D. and Akbar, Monika and Stewart, Michael and Ponce, Sean and Edwards, Stephen H.},
doi = {10.1145/1821996.1821997},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaffer et al. - 2010 - Algorithm Visualization.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {AlgoViz Wiki,Algorithm animation,algorithm visualization,community,data structure visualization,free and open source software},
month = {aug},
number = {3},
pages = {1--22},
publisher = {ACM},
title = {{Algorithm Visualization}},
url = {http://portal.acm.org/citation.cfm?doid=1821996.1821997},
volume = {10},
year = {2010}
}
@article{simonyan2013deep,
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
journal = {arXiv preprint arXiv:1312.6034},
title = {{Deep inside convolutional networks: Visualising image classification models and saliency maps}},
year = {2013}
}
@article{Smith2005,
abstract = {Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.},
author = {Smith, Noah A and Eisner, Jason},
doi = {10.3115/1219840.1219884},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith, Eisner - Unknown - Contrastive Estimation Training Log-Linear Models on Unlabeled Data.pdf:pdf},
isbn = {1932432515},
journal = {ACL'05 - 43rd Annual Meeting of the Association for Computational Linguistics},
pages = {354--362},
title = {{Contrastive estimation: Training Log-Linear Models on Unlabeled Data}},
url = {http://www.aclweb.org/anthology/P/P05/P05-1044.pdf},
year = {2005}
}
@inproceedings{sutskever14sequence,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {NIPS},
eprint = {1409.3215},
isbn = {1409.3215},
pages = {9},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@inproceedings{TjongKimSang2000,
address = {Morristown, NJ, USA},
author = {{Tjong Kim Sang}, Erik F. and Buchholz, Sabine},
booktitle = {Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning  -},
doi = {10.3115/1117601.1117631},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tjong Kim Sang, Buchholz - 2000 - Introduction to the CoNLL-2000 shared task.pdf:pdf},
pages = {127},
publisher = {Association for Computational Linguistics},
title = {{Introduction to the CoNLL-2000 shared task}},
url = {http://portal.acm.org/citation.cfm?doid=1117601.1117631},
volume = {7},
year = {2000}
}
@inproceedings{tzeng2005opening,
abstract = {Artificial neural networks are computer software or hardware models inspired by the structure and behavior of neurons in the human nervous system. As a powerful learning tool, increasingly neural networks have been adopted by many large-scale information processing applications but there is no a set of well defined criteria for choosing a neural network. The user mostly treats a neural network as a black box and cannot explain how learning from input data was done nor how performance can be consistently ensured. We have experimented with several information visualization designs aiming to open the black box to possibly uncover underlying dependencies between the input data and the output data of a neural network. In this paper, we present our designs and show that the visualizations not only help us design more efficient neural networks, but also assist us in the process of using neural networks for problem solving such as performing a classification task.},
author = {Tzeng, F-Y and Ma, K-L},
booktitle = {VIS 05. IEEE Visualization, 2005.},
doi = {10.1109/VISUAL.2005.1532820},
isbn = {0-7803-9462-3},
keywords = {arti cial neural network,classi cation,information visualization,machine learning,visualization application},
organization = {IEEE},
pages = {383--390},
title = {{Opening the black box - data driven visualization of neural networks}},
year = {2005}
}
@article{Vachovsky2016,
abstract = {The field of computer science su↵ers from a lack of diver-sity. The Stanford Artificial Intelligence Laboratory's Out-reach Summer (SAILORS), a two-week non-residential free summer program, recruits high school girls to computer sci-ence, specifically to Artificial Intelligence (AI). The program was organized by graduate student and professor volunteers. The goals of the pilot program are to increase interest in AI, contextualize technically rigorous AI concepts through soci-etal impact, and address barriers that could discourage 10th grade girls from pursuing computer science. In this paper we describe the curriculum designed to achieve these goals. Survey results show students had a statistically significant increase in technical knowledge, interest in pursuing careers in AI, and confidence in succeeding in AI and computer sci-ence. Additionally, survey results show that the majority of the students found new role models, faculty support, and a sense of community in AI and computer science.},
author = {Vachovsky, Marie E and Wu, Grace and Chaturapruek, Sorathan and Russakovsky, Olga and Sommer, Richard and Fei-Fei, Li and Wu, Grace},
doi = {10.1145/2839509.2844620},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vachovsky et al. - Unknown - Towards More Gender Diversity in CS through an Artificial Intelligence Summer Program for High School Girls.pdf:pdf},
isbn = {9781450336857},
journal = {Proceedings of the 47th ACM Technical Symposium on Computer Science Education (SIGCSE '16)},
keywords = {Artificial Intelligence,Com-puter Science Education,Gender and Diversity,K-12 Education,Outreach,Recruiting Women,Summer Camps,and diversity,artificial intelligence,com-,gender,k-12 education,outreach,puter science education,recruiting women,summer camps},
pages = {303--308},
title = {{Towards More Gender Diversity in CS through an Artificial Intelligence Summer Program for High School Girls}},
year = {2016}
}
@article{Verspoor,
author = {Verspoor, K and Nicholson, J},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verspoor, Nicholson - Unknown - e-Learning with Kaggle in Class Adapting the ALTA Shared Task 2013 to a Class Project.pdf:pdf},
journal = {Australasian Language Technology  {\ldots}},
title = {{e-Learning with Kaggle in Class: Adapting the ALTA Shared Task 2013 to a Class Project}},
url = {http://anthology.aclweb.org/U/U13/U13-1.pdf{\#}page=150}
}
@article{Vogel1996,
abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
author = {Vogel, Stephan and Ney, Hermann and Tillmann, Christoph},
doi = {10.3115/993268.993313},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vogel, Ney, Tillmann - Unknown - HMM-Based Word Alignment in Statistical Translation.pdf:pdf},
journal = {Proceedings of the 16th conference on Computational linguistics},
pages = {836--841},
title = {{HMM-based Word Alignment in Statistical Machine Translation}},
url = {http://portal.acm.org/citation.cfm?doid=993268.993313},
volume = {96pp},
year = {1996}
}
@inproceedings{Wang,
abstract = {We propose a novel data augmentation ap-proach to enhance computational behav-ioral analysis using social media text. In particular, we collect a Twitter corpus of the descriptions of annoying behaviors us-ing the {\#}petpeeve hashtags. In the qual-itative analysis, we study the language use in these tweets, with a special focus on the fine-grained categories and the ge-ographic variation of the language. In quantitative analysis, we show that lexi-cal and syntactic features are useful for au-tomatic categorization of annoying behav-iors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create addi-tional training instances significantly im-proves the lexical model; and incorporat-ing frame-semantic embedding achieves the best overall performance.},
author = {Wang, William Yang and Yang, Diyi},
booktitle = {EMNLP},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Yang - Unknown - That's So Annoying!!! A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Catego.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {2557--2563},
title = {{That ' s So Annoying !!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using {\#} petpeeve Tweets ∗}},
year = {2015}
}
@book{ware:2004:IVP,
address = {San Francisco},
author = {Ware, Colin},
doi = {http://dx.doi.org/10.1016/B978-155860819-1/50001-7},
edition = {2$\backslash$textsupe},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Information Visualization: Perception for Design}},
year = {2004}
}
@inproceedings{weiss15structured,
author = {Weiss, David and Alberti, Chris and Collins, Michael and Petrov, Slav},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, {\{}ACL{\}} 2015,},
pages = {323--333},
title = {{Structured Training for Neural Network Transition-Based Parsing}},
year = {2015}
}
@article{Weston2015,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {1410.3916v10},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
doi = {v0},
eprint = {1410.3916v10},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston, Chopra, Bordes - 2015 - MEMORY NETWORKS.pdf:pdf},
journal = {International Conference on Learning Representations},
keywords = {Neural Network: convolutional,Neural Network: recurrent,Neural network: memory},
pages = {1--14},
title = {{Memory Networks}},
url = {http://arxiv.org/abs/1410.3916},
year = {2015}
}
@misc{harvardwics,
author = {WiCS, Harvard},
title = {{Harvard Women in Computer Science Advocacy Report}},
url = {http://advocacy.harvardwics.com/},
urldate = {2016-06-30},
year = {2016}
}
@article{Wiseman2015,
abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa- tions, and we report the best overall score on the CoNLL 2012 English test set to date.},
author = {Wiseman, Sam and Rush, Alexander M and Shieber, Stuart M and Weston, Jason},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
pages = {1416--1426},
title = {{Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution}},
year = {2015}
}
@inproceedings{Wiseman2016,
abstract = {There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.},
archivePrefix = {arXiv},
arxivId = {1604.03035},
author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {To appear: NAACL-2016},
eprint = {1604.03035},
title = {{Learning Global Features for Coreference Resolution}},
url = {http://arxiv.org/abs/1604.03035},
year = {2016}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention(2).pdf:pdf},
journal = {ICML},
month = {feb},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@inproceedings{Yamada2001,
address = {Morristown, NJ, USA},
author = {Yamada, Kenji and Knight, Kevin},
booktitle = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics  - ACL '01},
doi = {10.3115/1073012.1073079},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamada, Knight - 2001 - A syntax-based statistical translation model.pdf:pdf},
pages = {523--530},
publisher = {Association for Computational Linguistics},
title = {{A syntax-based statistical translation model}},
url = {http://portal.acm.org/citation.cfm?doid=1073012.1073079},
year = {2001}
}
@article{DBLP:journals/corr/ZahavyBM16,
abstract = {In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.},
archivePrefix = {arXiv},
arxivId = {1602.02658},
author = {Zahavy, Tom and Zrihem, Nir Ben and Mannor, Shie},
eprint = {1602.02658},
journal = {arXiv},
title = {{Graying the black box: Understanding DQNs}},
url = {http://arxiv.org/abs/1602.02658},
volume = {abs/1602.0},
year = {2016}
}
@inproceedings{zeiler2014visualizing,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {Computer Vision–ECCV},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
organization = {Springer},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53$\backslash$nhttp://arxiv.org/abs/1311.2901$\backslash$npapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689},
year = {2014}
}
@article{Zhang2015,
abstract = {Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhao, Lecun - Unknown - Character-level Convolutional Networks for Text Classification.pdf:pdf},
isbn = {0123456789},
issn = {19909772},
journal = {NIPS},
keywords = {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion},
pages = {3057--3061},
title = {{Character-level Convolutional Networks for Text Classification}},
year = {2015}
}
@article{Zheng2015,
abstract = {Pixel-level labelling tasks, such as semantic segmenta-tion, play a central role in image understanding. Recent ap-proaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to de-lineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilis-tic graphical modelling. To this end, we formulate Con-ditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, mak-ing it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of seman-tic image segmentation, obtaining top results on the chal-lenging Pascal VOC 2012 segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03240v1},
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H S},
doi = {10.1109/ICCV.2015.179},
eprint = {arXiv:1502.03240v1},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zheng et al. - 2015 - Conditional Random Fields as Recurrent Neural Networks.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {International Conference on Computer Vision},
pages = {1529--1537},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Zheng{\_}Conditional{\_}Random{\_}Fields{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{economist,
journal = {The Economist},
title = {{Rise of the machines}},
year = {2015}
}

@article{nallapati2016sequence,
  title={Sequence-to-sequence rnns for text summarization},
    author={Nallapati, Ramesh and Xiang, Bing and Zhou, Bowen},
      journal={arXiv preprint arXiv:1602.06023},
        year={2016}
        }

@article{systran,
  title={SYSTRAN's Pure Neural Machine Translation System},
    author={Josep Crego and Jungi Kim and Jean Senellart},
      journal={arXiv preprint arXiv:1602.06023},
        year={2016}
        }